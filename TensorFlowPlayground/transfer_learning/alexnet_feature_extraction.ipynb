{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imageio import imread\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from caffe_classes import class_names\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the AlexNet param values\n",
    "net_data = np.load('bvlc-alexnet.npy', encoding='latin1').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def conv2d(input_vol, W, b, stride=1, padding='VALID', group=1):\n",
    "    c_i = input_vol.get_shape().as_list()[-1]\n",
    "    c_o = W.get_shape().as_list()[-1]\n",
    "    \n",
    "    assert(c_i % group == 0)\n",
    "    assert(c_o % group == 0)\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, strides=[1, stride, stride, 1], padding=padding)\n",
    "    \n",
    "    if tf.__version__ < \"1.0.0\":\n",
    "        if group == 1:\n",
    "            conv_layer = convolve(input_vol, W)\n",
    "        else:\n",
    "            input_groups = tf.split(3, group, input_vol)\n",
    "            kernel_groups = tf.split(3, group, W)\n",
    "            output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
    "            conv_layer = tf.concat(3, output_groups)\n",
    "    else:\n",
    "        if group == 1:\n",
    "            conv_layer = convolve(input_vol, W)\n",
    "        else:\n",
    "            input_groups = tf.split(input_vol, group, axis=3)\n",
    "            kernel_groups = tf.split(W, group, axis=3)\n",
    "            output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
    "            conv_layer = tf.concat(output_groups, axis=3)\n",
    "    \n",
    "    conv_layer = tf.nn.bias_add(conv_layer, b)\n",
    "    return tf.nn.relu(conv_layer)\n",
    "\n",
    "def maxpool2d(input_vol, k=2, stride=2):\n",
    "    return tf.nn.max_pool(input_vol, ksize=[1, k, k, 1],\n",
    "                          strides=[1, stride, stride, 1],\n",
    "                          padding='VALID')\n",
    "\n",
    "def alexnet(X, feature_extraction=False):\n",
    "    # CONV1 Layer\n",
    "    # Kernel=11x11x96. Strides=4x4. Group=1\n",
    "    conv1W = tf.Variable(net_data['conv1'][0])\n",
    "    conv1b = tf.Variable(net_data['conv1'][1])\n",
    "    conv1 = conv2d(X, conv1W, conv1b, stride=4, padding='SAME', group=1) \n",
    "    \n",
    "    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "    # NORM Layer\n",
    "    conv1 = tf.nn.local_response_normalization(conv1,\n",
    "                                               depth_radius=radius,\n",
    "                                               alpha=alpha,\n",
    "                                               beta=beta, bias=bias)     \n",
    "    # POOL Layer. Kernel=3x3. Strides=2x2.\n",
    "    conv1 = maxpool2d(conv1, 3)                                          \n",
    "    \n",
    "    # CONV2 Layer\n",
    "    # Kernel=5x5x256. Strides=1x1. Group=2\n",
    "    conv2W = tf.Variable(net_data['conv2'][0])\n",
    "    conv2b = tf.Variable(net_data['conv2'][1])\n",
    "    conv2 = conv2d(conv1, conv2W, conv2b, stride=1, padding='SAME', group=2) \n",
    "    \n",
    "    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "    # NORM Layer\n",
    "    conv2 = tf.nn.local_response_normalization(conv2,\n",
    "                                               depth_radius=radius,\n",
    "                                               alpha=alpha,\n",
    "                                               beta=beta, bias=bias)        \n",
    "    # POOL Layer. Kernel=3x3. Strides=2x2. \n",
    "    conv2 = maxpool2d(conv2, 3)                                             \n",
    "    \n",
    "    # CONV3 Layer\n",
    "    # Kernel=3x3x384. Strides=1x1. Group=1\n",
    "    conv3W = tf.Variable(net_data['conv3'][0])\n",
    "    conv3b = tf.Variable(net_data['conv3'][1])\n",
    "    conv3 = conv2d(conv2, conv3W, conv3b, stride=1, padding='SAME', group=1) \n",
    "    \n",
    "    # CONV4 Layer\n",
    "    # Kernel=3x3x384. Strides=1x1. Group=2\n",
    "    conv4W = tf.Variable(net_data['conv4'][0])\n",
    "    conv4b = tf.Variable(net_data['conv4'][1])\n",
    "    conv4 = conv2d(conv3, conv4W, conv4b, stride=1, padding='SAME', group=2) \n",
    "    \n",
    "    # CONV5 Layer\n",
    "    # Kernel=3x3x256. Strides=1x1. Group=2\n",
    "    conv5W = tf.Variable(net_data['conv5'][0])\n",
    "    conv5b = tf.Variable(net_data['conv5'][1])\n",
    "    conv5 = conv2d(conv4, conv5W, conv5b, stride=1, padding='SAME', group=2)\n",
    "    # POOL Layer. Kernel=3x3. Strides=2x2. \n",
    "    conv5 = maxpool2d(conv5, 3)            \n",
    "    \n",
    "    # FC6 Layer. 4096\n",
    "    fc6W = net_data['fc6'][0]\n",
    "    fc6b = net_data['fc6'][1]\n",
    "    fc6 = tf.contrib.layers.flatten(conv5)\n",
    "    fc6 = tf.nn.relu(tf.nn.xw_plus_b(fc6, fc6W, fc6b))\n",
    "    \n",
    "    # FC7 Layer. 4096\n",
    "    fc7W = net_data['fc7'][0]\n",
    "    fc7b = net_data['fc7'][1]\n",
    "    fc7 = tf.nn.relu(tf.nn.xw_plus_b(fc6, fc7W, fc7b))\n",
    "    \n",
    "    if feature_extraction:\n",
    "        return fc7\n",
    "    \n",
    "    # FC8 Layer. 1000\n",
    "    fc8W = net_data['fc8'][0]\n",
    "    fc8b = net_data['fc8'][1]\n",
    "    logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0\n",
      "miniature poodle: 0.3895\n",
      "toy poodle: 0.2231\n",
      "Bedlington terrier: 0.1730\n",
      "standard poodle: 0.1496\n",
      "komondor: 0.0258\n",
      "\n",
      "Image 1\n",
      "weasel: 0.3313\n",
      "polecat, fitch, foulmart, foumart, Mustela putorius: 0.2803\n",
      "black-footed ferret, ferret, Mustela nigripes: 0.2105\n",
      "mink: 0.0814\n",
      "Arctic fox, white fox, Alopex lagopus: 0.0268\n",
      "\n",
      "Time taken: 1.838\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Get the sample images for inference\n",
    "img1 = (imread(\"poodle.png\")[:, :, :3]).astype(np.float32)\n",
    "img2 = (imread(\"weasel.png\")[:, :, :3]).astype(np.float32)\n",
    "\n",
    "# Perform pre-processing\n",
    "img1 = img1 - np.mean(img1) # Range: [-127, +128]\n",
    "img2 = img2 - np.mean(img2) # Range: [-127, +128]\n",
    "\n",
    "# Define the placholders/hyperparams\n",
    "X = tf.placeholder(tf.float32, shape=[None, 227, 227, 3])\n",
    "\n",
    "# Define/Implement the model architecture\n",
    "probs = alexnet(X)\n",
    "\n",
    "# Perform the inference using the model\n",
    "start = time.time()\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    output = session.run(probs, feed_dict={X: [img1, img2]})\n",
    "    \n",
    "# Print the output\n",
    "for img_idx in range(output.shape[0]):\n",
    "    idxs = np.argsort(output[img_idx, :])\n",
    "    print('Image {}'.format(img_idx))\n",
    "    \n",
    "    for i in range(5):\n",
    "        print('{}: {:.4f}'.format(class_names[idxs[-1-i]], output[img_idx, idxs[-1 -i]]))\n",
    "    print()\n",
    "    \n",
    "print('Time taken: {:.3f}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on Traffic Sign Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0\n",
      "screen, CRT screen: 0.0510\n",
      "digital clock: 0.0408\n",
      "laptop, laptop computer: 0.0300\n",
      "balance beam, beam: 0.0270\n",
      "parallel bars, bars: 0.0227\n",
      "\n",
      "Image 1\n",
      "digital watch: 0.3954\n",
      "digital clock: 0.2749\n",
      "bottlecap: 0.1150\n",
      "stopwatch, stop watch: 0.1036\n",
      "combination lock: 0.0862\n",
      "\n",
      "Time taken: 0.729\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Get the sample images for inference\n",
    "img1 = (imread(\"construction.jpg\")[:, :, :3]).astype(np.float32)\n",
    "img2 = (imread(\"stop.jpg\")[:, :, :3]).astype(np.float32)\n",
    "\n",
    "# Perform pre-processing\n",
    "img1 = img1 - np.mean(img1) # Range: [-127, +128]\n",
    "img2 = img2 - np.mean(img2) # Range: [-127, +128]\n",
    "\n",
    "# Define the placholders/hyperparams\n",
    "X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "resized = tf.image.resize_images(X, [227, 227])\n",
    "\n",
    "# Define/Implement the model architecture\n",
    "probs = alexnet(resized)\n",
    "\n",
    "# Perform the inference using the model\n",
    "start = time.time()\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    output = session.run(probs, feed_dict={X: [img1, img2]})\n",
    "    \n",
    "# Print the output\n",
    "for img_idx in range(output.shape[0]):\n",
    "    idxs = np.argsort(output[img_idx, :])\n",
    "    print('Image {}'.format(img_idx))\n",
    "    \n",
    "    for i in range(5):\n",
    "        print('{}: {:.4f}'.format(class_names[idxs[-1-i]], output[img_idx, idxs[-1 -i]]))\n",
    "    print()\n",
    "    \n",
    "print('Time taken: {:.3f}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction using AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0\n",
      "Double curve: 0.4445\n",
      "Go straight or left: 0.3626\n",
      "Pedestrians: 0.0924\n",
      "Bicycles crossing: 0.0254\n",
      "Traffic signals: 0.0207\n",
      "\n",
      "Image 1\n",
      "Speed limit (50km/h): 0.5737\n",
      "Go straight or left: 0.3372\n",
      "Speed limit (20km/h): 0.0872\n",
      "Yield: 0.0015\n",
      "Children crossing: 0.0003\n",
      "\n",
      "Time taken: 0.707\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sign_names = pd.read_csv('signnames.csv')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Get the sample images for inference\n",
    "img1 = (imread(\"construction.jpg\")[:, :, :3]).astype(np.float32)\n",
    "img2 = (imread(\"stop.jpg\")[:, :, :3]).astype(np.float32)\n",
    "\n",
    "# Perform pre-processing\n",
    "img1 = img1 - np.mean(img1) # Range: [-127, +128]\n",
    "img2 = img2 - np.mean(img2) # Range: [-127, +128]\n",
    "\n",
    "# Define the placholders/hyperparams\n",
    "X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "resized = tf.image.resize_images(X, [227, 227])\n",
    "\n",
    "# Define/Implement the model architecture\n",
    "fc7 = alexnet(resized, feature_extraction=True)\n",
    "\n",
    "fc8_input_shape = fc7.get_shape().as_list()[-1]\n",
    "fc8W = tf.Variable(tf.truncated_normal([fc8_input_shape, 43], mean=0, stddev=0.1))\n",
    "fc8b = tf.Variable(tf.zeros([43]))\n",
    "logits = tf.nn.relu(tf.nn.xw_plus_b(fc7, fc8W, fc8b))\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "# Perform the inference using the model\n",
    "start = time.time()\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    output = session.run(probabilities, feed_dict={X: [img1, img2]})\n",
    "    \n",
    "# Print the output\n",
    "for img_idx in range(output.shape[0]):\n",
    "    idxs = np.argsort(output[img_idx, :])\n",
    "    print('Image {}'.format(img_idx))\n",
    "    \n",
    "    for i in range(5):\n",
    "        print('{}: {:.4f}'.format(sign_names.loc[idxs[-1-i]][1], output[img_idx, idxs[-1 -i]]))\n",
    "    print()\n",
    "    \n",
    "print('Time taken: {:.3f}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Feature Extractor for classification of images from the German Traffic Sign Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training examples: 26270\n",
      "# validation examples: 12939\n",
      "Training...\n",
      "\n",
      "Epoch: 1\n",
      "Validation accuracy: 0.8692\n",
      "\n",
      "Epoch: 2\n",
      "Validation accuracy: 0.9172\n",
      "\n",
      "Epoch: 3\n",
      "Validation accuracy: 0.9352\n",
      "\n",
      "Epoch: 4\n",
      "Validation accuracy: 0.9394\n",
      "\n",
      "Epoch: 5\n",
      "Validation accuracy: 0.9546\n",
      "\n",
      "Epoch: 6\n",
      "Validation accuracy: 0.9578\n",
      "\n",
      "Epoch: 7\n",
      "Validation accuracy: 0.9590\n",
      "\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.9611\n",
      "\n",
      "Epoch: 9\n",
      "Validation accuracy: 0.9646\n",
      "\n",
      "Epoch: 10\n",
      "Validation accuracy: 0.9661\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Get the data set\n",
    "with open('train.p', mode='rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['features'], data['labels'], test_size=0.33, random_state=0)\n",
    "\n",
    "# Data set summarisation\n",
    "print('# training examples: {}'.format(len(X_train)))\n",
    "print('# validation examples: {}'.format(len(X_val)))\n",
    "\n",
    "# Sanity checks\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_val) == len(y_val))\n",
    "\n",
    "# Define hyperparams\n",
    "IMAGE_SIZE = 227\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "N_LABELS = 43 # Number of labels for the GTS data set\n",
    "\n",
    "# Define placholders\n",
    "X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "X_resized = tf.image.resize_images(X, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "y = tf.placeholder(tf.int64, shape=[None])\n",
    "\n",
    "# Define and implement the architecture of the model\n",
    "fc7 = alexnet(X_resized, feature_extraction=True)\n",
    "fc7 = tf.stop_gradient(fc7)\n",
    "fc8_input_shape = fc7.get_shape().as_list()[-1]\n",
    "fc8W = tf.Variable(tf.truncated_normal([fc8_input_shape, N_LABELS], stddev=0.01))\n",
    "fc8b = tf.Variable(tf.zeros([N_LABELS]))\n",
    "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "\n",
    "# Training pipeline\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimiser = tf.train.AdamOptimizer().minimize(loss, var_list=[fc8W, fc8b])\n",
    "\n",
    "# Evaluation pipeline\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    nb_examples = len(X_data)\n",
    "    total_accuracy = 0.0\n",
    "\n",
    "    session = tf.get_default_session()\n",
    "    for offset in range(0, nb_examples, BATCH_SIZE):\n",
    "        X_batch, y_batch = X_data[offset: offset+BATCH_SIZE], y_data[offset: offset+BATCH_SIZE]\n",
    "        data_accuracy = session.run(accuracy, feed_dict={\n",
    "            X: X_batch,\n",
    "            y: y_batch\n",
    "        })\n",
    "\n",
    "        total_accuracy += (data_accuracy * len(X_batch))\n",
    "    return total_accuracy/nb_examples\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "# Start the training session\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    nb_examples = len(X_train)\n",
    "    for epoch_i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in tqdm(range(0, nb_examples, BATCH_SIZE)):\n",
    "            X_batch, y_batch = X_train[offset:offset+BATCH_SIZE], y_train[offset:offset+BATCH_SIZE]\n",
    "            session.run(optimiser, feed_dict={\n",
    "                X: X_batch,\n",
    "                y: y_batch\n",
    "            })\n",
    "\n",
    "        print('Epoch: {}'.format(epoch_i + 1))\n",
    "        print('Validation accuracy: {:.4f}'.format(evaluate(X_val, y_val)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "02cd9e167dfe410fb885502422acb22b": {
     "views": []
    },
    "033cc5770cdf4bb4b55d462796b26929": {
     "views": []
    },
    "03c81b3ef1844c7ab177f3b78c6fa75a": {
     "views": []
    },
    "046010d41fad415d81aea99a3eb11cff": {
     "views": []
    },
    "0484d450253d44ef9e1d6e9df651b06c": {
     "views": []
    },
    "08cd000e1d8d48cbaabba75d58fb6998": {
     "views": []
    },
    "091c1d7257cd49c7b92ff2c9e1dfae9c": {
     "views": []
    },
    "09efd4e332ee44e0841a43b3a69ca1f3": {
     "views": []
    },
    "0adb047d7e06424ab9d9961fe720925f": {
     "views": []
    },
    "0d1a0e944eca4a86a545841d963217ad": {
     "views": []
    },
    "0d9d50285f2749f29ed1e3dce3f52c87": {
     "views": []
    },
    "0e3abe924cda4a07a04669cb13abfec2": {
     "views": []
    },
    "0f47d98f70794c8691e7d0babdf7cfec": {
     "views": []
    },
    "0ff4c466ccb54e95b335aa27319a72fb": {
     "views": []
    },
    "10785f781d154ef08da866344cc3f5ed": {
     "views": []
    },
    "10f16735868d40328f852e48e87d369c": {
     "views": []
    },
    "12300cb8848542ca97fd3ddccd2cc755": {
     "views": []
    },
    "12b2bb3c8577428e94eaf93342acacf0": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "13bb8c01bf644a6cbb3e19e6c22ed9d2": {
     "views": []
    },
    "161237efc9d741cb906e394704833911": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "168cb76c5a5347c7aa31860d5b3aa344": {
     "views": []
    },
    "16c6fb1be2a3461ebd1aa5ceb2d92ab9": {
     "views": []
    },
    "18d1263e12a749e6907bbb7865e73e7e": {
     "views": []
    },
    "1943be89a36044aa9ba92029e34511a1": {
     "views": []
    },
    "19f2d886cf274e6091d14ec41798f8b3": {
     "views": []
    },
    "1a4c17f1254f4a45bbcc5479adf2f6e4": {
     "views": []
    },
    "1a5c318309d942a6992baf99231f3064": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "1be6ed8f333e4e39bcdad4ed77bd0abf": {
     "views": []
    },
    "1cdc5036c47c4fd4980e2a46751626b2": {
     "views": []
    },
    "1d7ea86e3514463bac14196e5084d99e": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "1e37e4db615843f892d718878cb6a1e3": {
     "views": []
    },
    "1efcf48c7f624b5daec45d0fa843455e": {
     "views": []
    },
    "20a3de87917b466db5cb66177ffe0d13": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "21300b17e4ef4878a5619ad82a47e10d": {
     "views": []
    },
    "21379e07c50e42dc86c100fbb5f06c26": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "21508e1d39e3491496088f7c40d26396": {
     "views": []
    },
    "21851fabd712424a9da7d70344ccf39f": {
     "views": []
    },
    "230aefa8124f465dbd0627cfc0339922": {
     "views": []
    },
    "24d69fe083f74d5e9509bde7a7279bd8": {
     "views": []
    },
    "25567caa586740a58a936ede0818663b": {
     "views": []
    },
    "2565ee9e98504e4fa49eeddf9a2cc920": {
     "views": []
    },
    "25c17f24a2824de590713ec0c5c6de17": {
     "views": []
    },
    "26603e23e9414996a3de36b92be31e1c": {
     "views": []
    },
    "2691bdf4e7cc4e839ab1e848f0345233": {
     "views": []
    },
    "2882899fe1d3440dae4edb7c1dc0d558": {
     "views": []
    },
    "2cf030205d2d4b7487677c9ce8fd885d": {
     "views": []
    },
    "2f6b1cff7e734b289bdba33941f5ab57": {
     "views": []
    },
    "3029e99e72484dbf881d4dc8fb6e5822": {
     "views": []
    },
    "32ca05af27314ebea23ebbfaad472c17": {
     "views": []
    },
    "32e6dc5bba974152bb7bdbff012277aa": {
     "views": []
    },
    "32fb3ec599154db4b6a34272b96277d9": {
     "views": []
    },
    "335a6f60d16944f7aad1d6c67b41c833": {
     "views": []
    },
    "33a83e4cd4284c5d80862bd3f2273d38": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "33dc703a84c44d9da3f81ba9349cda8c": {
     "views": []
    },
    "33f68406253a481f848965b1083f1e1c": {
     "views": []
    },
    "363b298442bf4661af6596eedab774a0": {
     "views": []
    },
    "36a3423d3e864164b004cebecd217aaa": {
     "views": []
    },
    "374764061b664adf9629030a6aff6657": {
     "views": []
    },
    "37d02d956b074d798cde94de06b06bb1": {
     "views": []
    },
    "39699f729db34242bea8cb9741824b07": {
     "views": []
    },
    "3986c92274c84f8e8a8707f713e7e050": {
     "views": []
    },
    "3b92fabb23c9423d838e15d08afe51f1": {
     "views": []
    },
    "3c97da3d1df54b06bc011f496b1ed34c": {
     "views": []
    },
    "3fbf6e313ffb40d4b4b2fcdae4d19653": {
     "views": []
    },
    "410e230dcdf0472fa02db4e064e9f936": {
     "views": []
    },
    "41363c30ae9349d9887569d4642a4712": {
     "views": []
    },
    "415fe264b156483b83ed87007dcf227d": {
     "views": []
    },
    "41b94f9d49af4f7fa3eb2b40d2bc0891": {
     "views": []
    },
    "41cd9bfd688e406a819aed8bdf5e58b0": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "442fdd3182b641db8a8273879c24e36c": {
     "views": []
    },
    "449c58c089f5441282a87e8e1bb1e9c6": {
     "views": []
    },
    "479443e50eac4a0a9b0cee6ddbfffc17": {
     "views": []
    },
    "484ca8cca7664b098651582e5249a275": {
     "views": []
    },
    "4b87fe12c78540de96d84f292cd5575b": {
     "views": []
    },
    "4bb2ac39a7cd4053a7bb72358bdcac9e": {
     "views": []
    },
    "4e7aec6e27944c33be349531dd13ee3a": {
     "views": []
    },
    "512b1825de3d4264803f5d90a6df26f3": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "519080908eff48508f55b7d56c824cc7": {
     "views": []
    },
    "5191417c95af4e629a916d80e903904c": {
     "views": []
    },
    "54d8e3b3a67f4a12a35b43d937e8df59": {
     "views": []
    },
    "565d56ef1e0641b49c1d80dee5a4a676": {
     "views": []
    },
    "585a9383d1524561b96a360bf358373a": {
     "views": []
    },
    "5af94520f9964ac3a3c6ab5becb7b62c": {
     "views": []
    },
    "5b6602de4c8b461ca4634447720b87ac": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "5db4b770fa0d4670bace55fceb994d64": {
     "views": []
    },
    "5e86645db4b24bd4adf5448094f9f334": {
     "views": []
    },
    "5fd10f87c63b4a449808ab87aa799ae9": {
     "views": []
    },
    "5ff58e2276ae4ac38532f1b7badd5583": {
     "views": []
    },
    "604976334be54c6bab9d73eaa676c6b5": {
     "views": []
    },
    "60ea7e8d103f438c9d61b260c09c344d": {
     "views": []
    },
    "62451ef6ab0d465bb77538b249cdc3c3": {
     "views": []
    },
    "62f191d087c640e4a7818b0297ea8711": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "63b622b56b3a4946bfa76fffc79f52dd": {
     "views": []
    },
    "64226a52596f4488844cc9f036c970aa": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "64668f1652db41208be2960b1676166f": {
     "views": []
    },
    "64c9ef0f3e01428086b536da98466dc0": {
     "views": []
    },
    "65c0b3b76aef467b8cf1821277c1c3ee": {
     "views": []
    },
    "6640dbbc1d5f4bceb819ea5de9ebf237": {
     "views": []
    },
    "696f3cff1549451181991800a83b1dac": {
     "views": []
    },
    "69de294895fa4e6ab0b3c5d2489c0cb8": {
     "views": []
    },
    "6ae670ca75e44cc489b8cd3d023d811a": {
     "views": []
    },
    "6c0118717d0f4a5aafd6194de80d76f6": {
     "views": []
    },
    "6c1665df0b3c474eb2d95277da347201": {
     "views": []
    },
    "6e90c8ceb7dd4d76992a5cb884cd04d6": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6febde9b2046481ba53a19f66558769e": {
     "views": []
    },
    "712f6e2efe18496883975898c6f03459": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "715dc1131c61489a9c5e6267d7c351ff": {
     "views": []
    },
    "71ea4e8ddd3140ea88f6c6f7208f6fad": {
     "views": []
    },
    "73699a36856e4585ba78b23ea9ce850b": {
     "views": []
    },
    "7548131380cb4ed799f7450db4702e50": {
     "views": []
    },
    "755dc51feed14ce8a0408f62635dcf3b": {
     "views": []
    },
    "7634e2b781ef43838b3fcb683b266f79": {
     "views": []
    },
    "789ea65239f642939aa2f0c6780b82a4": {
     "views": []
    },
    "790830686eeb4cfba65c0999779413c0": {
     "views": []
    },
    "79c70b4f06f849ad99d4894ee0100669": {
     "views": []
    },
    "7ba2a8f189994399b989e66ac1315b12": {
     "views": []
    },
    "7bbe8a12f1d7453099710376e547259a": {
     "views": []
    },
    "7c1c070ddabe42c79579aa638c55e650": {
     "views": []
    },
    "7c85cc2d92b744ed9d0856bcb681055b": {
     "views": []
    },
    "7dbb3f6dda844fd7be622116d07a331a": {
     "views": []
    },
    "80c8727fc9934927bcca04fdf17a9b8d": {
     "views": []
    },
    "81f82e4c0c8c485fbde92c9bc55a6899": {
     "views": []
    },
    "82a3eb189508429db76218e46afb589d": {
     "views": []
    },
    "85e51333bbdc415fa197e114f13e83aa": {
     "views": []
    },
    "87a6a036d9154a38bf8417a9464f469c": {
     "views": []
    },
    "8a2bc5954f7d4107a5b1f36faf0355dc": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "8c48b88d9ebb43faa5e66e447b06e8a6": {
     "views": []
    },
    "8d6accfd19194a47ba12e7d84bf38864": {
     "views": []
    },
    "8d7b437530914b6da62ba8ced7200efc": {
     "views": []
    },
    "9041d1c07e6945869cce8ea6bcc1966c": {
     "views": []
    },
    "935766fb34804d77b1c7129c2fa6ca42": {
     "views": []
    },
    "94b9acf280414db9a428b770edb51c22": {
     "views": []
    },
    "995744f751bd4d8eb72530844e881f33": {
     "views": []
    },
    "9a1f4e503ee94eddbff73c3452757655": {
     "views": []
    },
    "9a90a13e42c742e9854066a0317de894": {
     "views": []
    },
    "9b2065970a4c40d8b06ab83bbb91d813": {
     "views": []
    },
    "9bd760dfe6144c00a332b35e7d69bf2c": {
     "views": []
    },
    "9c6b8a9d332942d79ae0e9f92549c379": {
     "views": []
    },
    "9fe940dda1a148fcb1df4f9b9ba54497": {
     "views": []
    },
    "a00d4a77f1d84af5ad4fe69f7ccb47f5": {
     "views": []
    },
    "a08ae42e3837483c8596e2ac8321e3bf": {
     "views": []
    },
    "a2998a9a83114906be0cdecc221ce3d8": {
     "views": []
    },
    "a2f0714e58c54ae1b4f9c5ccca00c2ac": {
     "views": []
    },
    "a33a0f0374304438b68086664f658fcf": {
     "views": []
    },
    "a4ed9667739f418dab83cd9df8d9eebc": {
     "views": []
    },
    "a533a40e58c94ebe9f968a814e1209c0": {
     "views": []
    },
    "a6c1a88865e841ae8feee3ab117bd604": {
     "views": []
    },
    "ab41cb2fda0a4328b1c85c0833bda594": {
     "views": []
    },
    "ab6bd8de31b94620ab1ceea52ebda0bd": {
     "views": []
    },
    "abf32896fa6648859e135182c18cf17a": {
     "views": []
    },
    "acfe94a19eba41caa5312f23537e5312": {
     "views": []
    },
    "ae75e15d7e4f4aa086ee8fef1c11fb3f": {
     "views": []
    },
    "aed458bf1e3544ca804cad680457998a": {
     "views": []
    },
    "aef0d1415a19450c93f0f40b840e6ef1": {
     "views": []
    },
    "af941c99b2f14a9592e538d640b7dcff": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "b04640fd2dcf4783850c3c35cd732433": {
     "views": []
    },
    "b07b8e3520cf4e719f1b5ac47807e71b": {
     "views": []
    },
    "b0aba1d48e79404aba77f7a062f82cf0": {
     "views": []
    },
    "b645fb67436e40968d92ac09a53b5fd6": {
     "views": []
    },
    "b69a81c521764ebbaf5e81053baa0360": {
     "views": []
    },
    "b70e2e416fd142edadf22f14f021419d": {
     "views": []
    },
    "b71dd303de9d48f790fe55822de0789e": {
     "views": []
    },
    "b745c825f3e2453ca803618251d4f264": {
     "views": []
    },
    "b85d7edeae164806ac9eda9ebdf19710": {
     "views": []
    },
    "b89acd7f676f49fd9945888bb0830127": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "b90774f0d8ce4f8d900b6d540d2e45a0": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "b920da660d4c4651b884e806b6ba059f": {
     "views": []
    },
    "bc63b641791f470996f4eaa178e27854": {
     "views": []
    },
    "bc8f30999850447d9a7b8e93153f0c5e": {
     "views": []
    },
    "bd0ad0068dfb45db85de0a1f3364ae91": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "be145d7b2a924530ac339d0ce2987732": {
     "views": []
    },
    "be369178954444ddac60491f04e09561": {
     "views": []
    },
    "c1bf673265184c33b60f86fd5b2e432b": {
     "views": []
    },
    "c1c3da82308f4e6ca1c04e287bd2c2d7": {
     "views": []
    },
    "c39111e25503466cba931827276bdec9": {
     "views": []
    },
    "c6cb20e4de204c25a4686b46960e5818": {
     "views": []
    },
    "c6d5a216037a404cbc9db44237daa1a7": {
     "views": []
    },
    "c6e66bb2f94c4247838e81f369952852": {
     "views": []
    },
    "c7ba6efd80d04e9aafdf8c6a8b6809ec": {
     "views": []
    },
    "c83d99b212a3409788ecd3a15e99c46c": {
     "views": []
    },
    "c91d849d8bef47359a70415c6adcf4b7": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "cae2d78a10174be4b971f237119f75bb": {
     "views": []
    },
    "cc27b5f9e40f46768f61ef6a61a5f4db": {
     "views": []
    },
    "cc734e46301f48c7b0990b54bfd68c45": {
     "views": []
    },
    "ccc25e7e156e43c58a2106931962a2f0": {
     "views": []
    },
    "cdba32056a98454bb4bde37d6a1116a7": {
     "views": []
    },
    "ce11480cb4254921a56200ea74afca32": {
     "views": []
    },
    "cee148b0d97941859765f4cb9406c33b": {
     "views": []
    },
    "d13dab8a81984e27b51d868344556b0a": {
     "views": []
    },
    "d219d668d3314a40ac3817ff281ed14d": {
     "views": []
    },
    "d34f6fb0c9e44ee4a2ff7d53854e3431": {
     "views": []
    },
    "d417d61d4d684951b4c4c74190cf92ab": {
     "views": []
    },
    "d59bf664e1f6495d895b5d8004bcec05": {
     "views": []
    },
    "d6ae316e059642cb93a855c477d7e7a7": {
     "views": []
    },
    "d6fba178a4f44aecb8fa026997fe9137": {
     "views": []
    },
    "d76d510fffcf45bcbc3a31ec69374c6d": {
     "views": []
    },
    "da8451ce63f14ce689e463cc1d4020d0": {
     "views": []
    },
    "e1b3ef7569734076a5e53eba34e5b614": {
     "views": []
    },
    "e34f426b6a4c4a9a84542d1488ba155b": {
     "views": []
    },
    "ea1c4544dfbc4f858ee392ebd29931d9": {
     "views": []
    },
    "ebe088912db141329f60886fd32cade7": {
     "views": []
    },
    "ec9caf1517524a94a14051d2262d230d": {
     "views": []
    },
    "ed414ca4a8194840a6e044ea09179b57": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "eeec5dcedb8140ca8df87d4e11723ddc": {
     "views": []
    },
    "ef0bb1ec5ef44fa6a74d425a78d42e61": {
     "views": []
    },
    "f0ddc799b856407fb8ffbfeae87bb201": {
     "views": []
    },
    "f19778111fc04888a0a6ad8570ebf06a": {
     "views": []
    },
    "f1b9cfabb4a3496cacba36725dd957b5": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "f295a4aa1eeb43e49e5e8f5757434db4": {
     "views": []
    },
    "f447a9344d8f4aedb0ab3abf29543f2d": {
     "views": []
    },
    "f65885dff08b4377ad6cb915bb217769": {
     "views": []
    },
    "f689bb001c214afaac9c3f72230551d5": {
     "views": []
    },
    "f80cd016a01240efb9e49868aa13c92d": {
     "views": []
    },
    "fb506441789a46fb807a58f0e5fcd391": {
     "views": []
    },
    "fc41733f724448338a4865e50a8ae193": {
     "views": []
    },
    "fff0dad9c3464e43b2e3d658dd75b828": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
