{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet Implementation\n",
    "\n",
    "![LeNet Architecture](lenet.png)\n",
    "\n",
    "Source: Yan LeCun\n",
    "\n",
    "### Load the MNIST Data\n",
    "Load the MNIST Data that comes pre-loaded with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_dataset/train-images-idx3-ubyte.gz\n",
      "Extracting mnist_dataset/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist_dataset/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_dataset/t10k-labels-idx1-ubyte.gz\n",
      "Image shape: (28, 28, 1)\n",
      "# Training samples: 55000\n",
      "# Validation samples: 5000\n",
      "# Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"mnist_dataset/\", one_hot=True, reshape=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_validation) == len(y_validation))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "print('Image shape: {}'.format(X_train[0].shape))\n",
    "print('# Training samples: {}'.format(len(X_train)))\n",
    "print('# Validation samples: {}'.format(len(X_validation)))\n",
    "print('# Test samples: {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The MNIST data that TensorFlow pre-loads comes as 28x28x1 images.\n",
    "\n",
    "However, the LeNet architecture only accepts 32x32xC images, where C is the number of color channels.\n",
    "\n",
    "In order to reformat the MNIST data into a shape that LeNet will accept, we pad the data with two rows of zeros on the top and bottom, and two columns of zeros on the left and right (28+2+2 = 32).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated Image shape is: (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Pad images with 0s\n",
    "X_train = np.pad(X_train, [[0,0], [2,2], [2,2], [0,0]], 'constant')\n",
    "X_validation = np.pad(X_validation, [[0,0], [2,2], [2,2], [0,0]], 'constant')\n",
    "X_test = np.pad(X_test, [[0,0], [2,2], [2,2], [0,0]], 'constant')\n",
    "\n",
    "print('The updated Image shape is: {}'.format(X_train[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2827c2318d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = np.random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap='gray')\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.005\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = tf.placeholder(tf.float32, shape=[None, 32, 32, 1])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "weights = {\n",
    "    'c1': tf.Variable(tf.truncated_normal([5, 5, 1, 6])),\n",
    "    'c2': tf.Variable(tf.truncated_normal([5, 5, 6, 16])),\n",
    "    'fc1': tf.Variable(tf.truncated_normal([400, 120])),\n",
    "    'fc2': tf.Variable(tf.truncated_normal([120, 84])),\n",
    "    'out': tf.Variable(tf.truncated_normal([84, 10]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'c1': tf.Variable(tf.truncated_normal([6])),\n",
    "    'c2': tf.Variable(tf.truncated_normal([16])),\n",
    "    'fc1': tf.Variable(tf.truncated_normal([120])),\n",
    "    'fc2': tf.Variable(tf.truncated_normal([84])),\n",
    "    'out': tf.Variable(tf.truncated_normal([10]))\n",
    "}\n",
    "\n",
    "def conv2d(input_vol, W, b, stride=1, padding='SAME'):\n",
    "    conv_layer = tf.nn.conv2d(input_vol, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, b)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    return conv_layer\n",
    "\n",
    "def pool2d(input_vol, k=2, stride=2):\n",
    "    return tf.nn.max_pool(input_vol, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def lenet(images, weights, biases, keep_probab):\n",
    "    # CONV1 Input = [32x32x1] Output = [28x28x6]\n",
    "    conv1 = conv2d(images, weights['c1'], biases['c1'], padding='VALID')\n",
    "    \n",
    "    # POOL1 Input = [28x28x6] Output = [14x14x6]\n",
    "    pool1 = pool2d(conv1)\n",
    "    \n",
    "    # CONV2 Input = [14x14x6] Output = [10x10x16]\n",
    "    conv2 = conv2d(pool1, weights['c2'], biases['c2'], padding='VALID')\n",
    "    \n",
    "    # POOL2 Input = [10x10x16] Output = [5x5x16]\n",
    "    pool2 = pool2d(conv2)\n",
    "    \n",
    "    # FC1 Input = 400 Output = 120\n",
    "    fc1 = tf.reshape(pool2, shape=[-1, weights['fc1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.matmul(fc1, weights['fc1']) + biases['fc1']\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    # FC2 Input = 120 Output = 84\n",
    "    fc2 = tf.matmul(fc1, weights['fc2']) + biases['fc2']\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # OUT Input = 84 Output = 10\n",
    "    out = tf.matmul(fc2, weights['out']) + biases['out']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = lenet(images, weights, biases, keep_prob)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate)\\\n",
    "                .minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, dtype=tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    session = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        batch_x, batch_y = X_data[offset: offset+batch_size], y_data[offset: offset+batch_size]\n",
    "        data_accuracy = session.run(accuracy, feed_dict={\n",
    "            images: batch_x,\n",
    "            labels: batch_y,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "        total_accuracy += (data_accuracy * batch_size)\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "EPOCH 0 ...\n",
      "Validation Accuracy = 0.931\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.949\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.963\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.977\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.977\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.978\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.988\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.981\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.986\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.987\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('Training...')\n",
    "    num_examples = len(X_train)\n",
    "          \n",
    "    for epoch_i in range(num_epochs):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            session.run(optimiser, feed_dict={images: batch_x, labels: batch_y, keep_prob: 0.5})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(epoch_i))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(session, 'lenet/lenet.ckpt')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from lenet/lenet.ckpt\n",
      "Test Accuracy = 0.975\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('lenet/'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
