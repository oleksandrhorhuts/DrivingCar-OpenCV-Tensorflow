{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5 Implementation\n",
    "\n",
    "![LeNet Architecture](lenet.png)\n",
    "\n",
    "Source: Yan LeCun\n",
    "\n",
    "### Load the MNIST Data\n",
    "Load the MNIST Data that comes pre-loaded with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_dataset/train-images-idx3-ubyte.gz\n",
      "Extracting mnist_dataset/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist_dataset/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_dataset/t10k-labels-idx1-ubyte.gz\n",
      "Image shape: (28, 28, 1)\n",
      "# Training samples: 55000\n",
      "# Validation samples: 5000\n",
      "# Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"mnist_dataset/\", one_hot=True, reshape=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_validation) == len(y_validation))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "print('Image shape: {}'.format(X_train[0].shape))\n",
    "print('# Training samples: {}'.format(len(X_train)))\n",
    "print('# Validation samples: {}'.format(len(X_validation)))\n",
    "print('# Test samples: {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The MNIST data that TensorFlow pre-loads comes as 28x28x1 images.\n",
    "\n",
    "However, the LeNet architecture only accepts 32x32xC images, where C is the number of color channels.\n",
    "\n",
    "In order to reformat the MNIST data into a shape that LeNet will accept, we pad the data with two rows of zeros on the top and bottom, and two columns of zeros on the left and right (28+2+2 = 32).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated Image shape is: (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Pad images with 0s\n",
    "X_train = np.pad(X_train, [[0,0], [2,2], [2,2], [0,0]], 'constant')\n",
    "X_validation = np.pad(X_validation, [[0,0], [2,2], [2,2], [0,0]], 'constant')\n",
    "X_test = np.pad(X_test, [[0,0], [2,2], [2,2], [0,0]], 'constant')\n",
    "\n",
    "print('The updated Image shape is: {}'.format(X_train[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABTBJREFUeJztnF+IVFUcxz/f3PKlFTazlEYqInx0R6MVehCNIELY9qHI\nhyhYMQRhe1CSBaEHH3zYejRwTXwJlqBQYZXoIZBYDW2R0sRZiZx2m3XMHmpDXKpfDzOz7uzfu3Nn\nfnf2ej5wuXvPnTvny/f++O05Z845MjMCPjyUtIAHiWC2I8FsR4LZjgSzHQlmOxLMdiSW2ZJelXRd\n0g1JB+olKq2o1k6NpBVADngFGAUuAjvN7Kf6yUsXLTGefRG4YWY/A0gaADqBec2WlNruqplpsc/E\nSSNPAb9Oux4tl1UhabekS5IuxagrFcSJ7Lne5KzINbOjwFFId2RHIU5kjwLrp11ngN/iyUk3ccy+\nCDwv6VlJjwBvAafrIyud1JxGzOwfSXuBr4AVwHEzu1o3ZSmk5qZfTZWlOGc3ujUSWCLBbEeC2Y4E\nsx0JZjuSSrNPnjxJPp8nn88nLaWKVJrdrMQZG2k6Vq9eDUBHRweTk5NV97q6ugAYGRnhypUr7tog\nZWZv374dgDVr1jA2NlZ1b9++fQAMDQ2xf/9+d20Q0ogrqYrsXbt2JS1hQUJkO5KKyF65ciVw/x+k\nJKTSuNCBA6Xfobds2QLA+fPnE1BYIkS2I6mI7Gw2C0B7ezsAZsbQ0BAAe/bsmSoDKBQKCSgskQqz\nN2zYUHWdy+Xo6+sD4MKFCwCMjo4CcOLECVdt0wlpxJFlH9mtra309PRUlQ0ODrJjx46qsiNHjgBw\n584dN20zCZHtyLKP7M7OTjZu3FhVtmrVKrZt2wYw1QSsnJNk2ZudzWaZ+aN1d3f3lLmVe5VWyfDw\nMJs3b556FmBgYACAM2fOcO/evYZpDWnEkWUf2VHJZDIAnD17dlbUd3R0AHDu3LkQ2WnhgYnshfBq\nFi4a2ZLWS/pG0jVJVyX1lMsfk/S1pJHyua2hSlPAotPPJK0D1pnZsKRW4HvgdeBd4A8zO1xe4tFm\nZh8s8l11n362detWNm3aVFV28OBB2tpK7/727dsA3L17F4Bjx44xMTEB3B8nqbRG4hBl+hlmtqQD\nOEVpacd1Si8BYB1wPcKz5nHcvHnTisWiFYtFy2QylslkGl5nFO+WlLMlPQNkge+AJ82sUH5hBUlP\nzPPMbmD3UupJK5HNlvQo8AXwvpn9GbVHltTKg0raqIz2NQORmn6SHqZk9Gdm9mW5+FY5n1fyerEx\nEtPDopGtUgh/Clwzs4+n3ToNvAMcLp9PNUThEqiM9K1du5bx8fGE1cwmShp5CXgb+FHS5XJZLyWT\nP5fUDeSBNxojMT0saraZfcvcK8MAXq6vnHj09vYC0NLSwqFDhxJWM5tU9SArfQYzo7+/P2E1swlj\nI46k0uxcLpe0hDlJpdnNSirNHhwcTFrCnIR1kHUirINsMoLZjgSzHQlmOxLMdiSY7Ugw25FgtiPe\no36/A3+Xz83O40TX+XSUD7n2IAEkXTKzF1wrrYFG6AxpxJFgtiNJmH00gTproe463XP2g0xII464\nmd3Me20vMFP3Q0ljki6Xj9di1eORRpp9r+0FZuq+CUyYWV896vGK7Km9ts1sEqjstd0UmFnBzIbL\nf/8FXGOO7anj4mV2pL22m4EZM3UB9kr6QdLxuBP+vcyOtNd20sycqQt8AjwHtAMF4KM43+9ldtPv\ntT3XTF0zu2Vm/5rZf0A/pXRYM15mN/Ve2/PN1K1MiS7TBcTaNs1l1G8Z7LU930zdnZLaKaW8X4D3\n4lQSepCOhB6kI8FsR4LZjgSzHQlmOxLMdiSY7Ugw25H/AeReONZlCGyfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126385748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = np.random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap='gray')\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.005\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = tf.placeholder(tf.float32, shape=[None, 32, 32, 1])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "weights = {\n",
    "    'c1': tf.Variable(tf.truncated_normal([5, 5, 1, 6])),\n",
    "    'c2': tf.Variable(tf.truncated_normal([5, 5, 6, 16])),\n",
    "    'fc1': tf.Variable(tf.truncated_normal([400, 120])),\n",
    "    'fc2': tf.Variable(tf.truncated_normal([120, 84])),\n",
    "    'out': tf.Variable(tf.truncated_normal([84, 10]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'c1': tf.Variable(tf.truncated_normal([6])),\n",
    "    'c2': tf.Variable(tf.truncated_normal([16])),\n",
    "    'fc1': tf.Variable(tf.truncated_normal([120])),\n",
    "    'fc2': tf.Variable(tf.truncated_normal([84])),\n",
    "    'out': tf.Variable(tf.truncated_normal([10]))\n",
    "}\n",
    "\n",
    "def conv2d(input_vol, W, b, stride=1, padding='SAME'):\n",
    "    conv_layer = tf.nn.conv2d(input_vol, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, b)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    return conv_layer\n",
    "\n",
    "def pool2d(input_vol, k=2, stride=2):\n",
    "    return tf.nn.max_pool(input_vol, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def lenet(images, weights, biases, keep_probab):\n",
    "    # CONV1 Input = [32x32x1] Output = [28x28x6]\n",
    "    conv1 = conv2d(images, weights['c1'], biases['c1'], padding='VALID')\n",
    "    \n",
    "    # POOL1 Input = [28x28x6] Output = [14x14x6]\n",
    "    pool1 = pool2d(conv1)\n",
    "    \n",
    "    # CONV2 Input = [14x14x6] Output = [10x10x16]\n",
    "    conv2 = conv2d(pool1, weights['c2'], biases['c2'], padding='VALID')\n",
    "    \n",
    "    # POOL2 Input = [10x10x16] Output = [5x5x16]\n",
    "    pool2 = pool2d(conv2)\n",
    "    \n",
    "    # FC1 Input = 400 Output = 120\n",
    "    fc1 = tf.reshape(pool2, shape=[-1, weights['fc1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.matmul(fc1, weights['fc1']) + biases['fc1']\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    # FC2 Input = 120 Output = 84\n",
    "    fc2 = tf.matmul(fc1, weights['fc2']) + biases['fc2']\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # OUT Input = 84 Output = 10\n",
    "    out = tf.matmul(fc2, weights['out']) + biases['out']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = lenet(images, weights, biases, keep_prob)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate)\\\n",
    "                .minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, dtype=tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    session = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        batch_x, batch_y = X_data[offset: offset+batch_size], y_data[offset: offset+batch_size]\n",
    "        data_accuracy = session.run(accuracy, feed_dict={\n",
    "            images: batch_x,\n",
    "            labels: batch_y,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "        total_accuracy += (data_accuracy * batch_size)\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "EPOCH 0 ...\n",
      "Validation Accuracy = 0.892\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.943\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.971\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.973\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.971\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.977\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.981\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.986\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.989\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('Training...')\n",
    "    num_examples = len(X_train)\n",
    "          \n",
    "    for epoch_i in range(num_epochs):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            session.run(optimiser, feed_dict={images: batch_x, labels: batch_y, keep_prob: 0.5})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(epoch_i))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(session, 'lenet_ckpts/lenet.ckpt')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from lenet_ckpts/lenet.ckpt\n",
      "Test Accuracy = 0.974\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('lenet_ckpts/'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
