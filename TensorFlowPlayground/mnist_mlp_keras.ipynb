{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.2719 - acc: 0.9164 - val_loss: 0.1236 - val_acc: 0.9617\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1122 - acc: 0.9657 - val_loss: 0.1076 - val_acc: 0.9665\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0799 - acc: 0.9754 - val_loss: 0.0917 - val_acc: 0.9730\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0626 - acc: 0.9805 - val_loss: 0.0797 - val_acc: 0.9781\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0521 - acc: 0.9842 - val_loss: 0.0848 - val_acc: 0.9785\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0445 - acc: 0.9865 - val_loss: 0.0882 - val_acc: 0.9778\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0377 - acc: 0.9884 - val_loss: 0.0954 - val_acc: 0.9778\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0351 - acc: 0.9898 - val_loss: 0.0925 - val_acc: 0.9799\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0293 - acc: 0.9914 - val_loss: 0.1165 - val_acc: 0.9772\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0278 - acc: 0.9915 - val_loss: 0.0998 - val_acc: 0.9792\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0257 - acc: 0.9928 - val_loss: 0.1013 - val_acc: 0.9808\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0232 - acc: 0.9930 - val_loss: 0.1038 - val_acc: 0.9804\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.0224 - acc: 0.9936 - val_loss: 0.1089 - val_acc: 0.9789\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0202 - acc: 0.9941 - val_loss: 0.1127 - val_acc: 0.9820\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0204 - acc: 0.9945 - val_loss: 0.1099 - val_acc: 0.9812\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0210 - acc: 0.9943 - val_loss: 0.1268 - val_acc: 0.9794\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0196 - acc: 0.9948 - val_loss: 0.1213 - val_acc: 0.9806\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0149 - acc: 0.9954 - val_loss: 0.1197 - val_acc: 0.9817\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0167 - acc: 0.9956 - val_loss: 0.1211 - val_acc: 0.9820\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.0164 - acc: 0.9957 - val_loss: 0.1273 - val_acc: 0.9811\n",
      "Test loss: 0.101684820792\n",
      "Test accuracy: 0.9837\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
